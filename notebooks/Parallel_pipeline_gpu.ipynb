{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316f38c-d218-41ad-a60d-c612a2f08949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "%cd ..\n",
    "from utils import get_data,ColumnsPreprocessing,get_specific_df,ColumnsTrainKfold,seedEverything,grid_parameters_name,grid_parameters,TrainOneFold,get_cv_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from stg_fs import get_stg_class,get_SelectFdr_class,get_mrmr_class,get_reliefF_class,get_RFE_SVM_class,get_FWDT_class,get_ensemble_class, get_ensemble_class_new ,get_stg_class_new,get_FWDT_class_new\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "from calculate_metric_score import get_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4449a-8fb7-417d-ba80-22512bc37894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_GridSearchCV(train_features,y,fun,name,cv=3):\n",
    "    if os.path.exists(f'output/Preprocessing/{name}.pickle'):\n",
    "        with open(f'output/Preprocessing/{name}.pickle', 'rb') as handle:\n",
    "            results = pickle.load(handle)\n",
    "        return results\n",
    "    results = {}\n",
    "    for n,params in tqdm(zip(grid_parameters_name,grid_parameters)):\n",
    "        clf = params['clf'][0]\n",
    "        params2 = params.copy()\n",
    "        params2.pop('clf')\n",
    "        estimators = [(\"SelectKBest\",SelectKBest(fun,k=100)), ('clf', clf)]\n",
    "        pipe = Pipeline(estimators)\n",
    "        grid = GridSearchCV(pipe, param_grid=params2, cv=cv,n_jobs=-1)\n",
    "        # print(train_features.shape)\n",
    "        # print(y.shape)\n",
    "        # print(params2)\n",
    "        _=grid.fit(train_features,y)\n",
    "        results[n] = {}\n",
    "        for row in grid.best_params_:\n",
    "            results[n][row.split(\"__\")[-1]] = grid.best_params_[row]\n",
    "    with open(f'output/Preprocessing/{name}.pickle', 'wb') as handle:\n",
    "        pickle.dump(results, handle, protocol=4)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e134e8-471b-4379-8f02-c1162329018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kfold(get_stg_fun,\n",
    "                            Filtering_Algorithm,\n",
    "                            k,\n",
    "                            datsets_num,\n",
    "                            f_outpot,\n",
    "                            end,\n",
    "                            knn_args = {\"n_neighbors\": 5},\n",
    "                            rf_args = {\"n_estimators\":100},\n",
    "                            lr_args = {\"C\":1e5},\n",
    "                            SVC_args = {\"C\":1,\"probability\":True},\n",
    "                            NB_args = {\"alpha\":1}):\n",
    "\n",
    "    knn = KNeighborsClassifier(**knn_args)\n",
    "    rf = RandomForestClassifier(**rf_args)\n",
    "    lr = LogisticRegression(**lr_args)\n",
    "    svc = SVC(**SVC_args)\n",
    "    BN = BernoulliNB(**NB_args)\n",
    "    models = {\"knn\":knn,\"rf\":rf,\"lr\":lr,\"SVC\":svc,\"BN\":BN}\n",
    "    history = {}\n",
    "    df,name = get_specific_df(datsets_num)\n",
    "    skf,n_splits,name_cv = get_cv_split(df)\n",
    "    train_features,y = PP.transform(df)\n",
    "    start = time.time()\n",
    "    score_gates = get_stg_fun(train_features.values,y.values,k=k)\n",
    "    topk = score_gates.argsort()[::-1][0:k]\n",
    "    col_name = np.array(df.drop('target',axis=1).columns)[topk]\n",
    "    score_gates = score_gates[topk]\n",
    "    history[f\"time_gates\"] = end \n",
    "    history[f\"score_gates\"] = score_gates\n",
    "    history[f\"col_name\"] = col_name\n",
    "    \n",
    "    score_dict = {}\n",
    "    run_only_cv = False if \"Folds\" in name_cv else True\n",
    "    split_fun = skf.split(train_features, y) if \"Folds\" in name_cv else skf.split(train_features)\n",
    "    train_features = train_features.values\n",
    "    y = y.values\n",
    "    for clf in models:\n",
    "        history[clf] = {}\n",
    "        history[clf]['score'] = {}\n",
    "        history[clf]['y_score'] = []\n",
    "        history[clf]['y_val'] = []\n",
    "        history[clf]['index_val'] = []\n",
    "    for fold,(train_index, test_index) in enumerate(tqdm(split_fun)):\n",
    "        X_train, X_test = train_features[train_index].copy(), train_features[test_index].copy()\n",
    "        y_train, y_test = y[train_index].copy(), y[test_index].copy()\n",
    "        if not run_only_cv:\n",
    "            out = f_outpot[fold]\n",
    "            get_stg_fun = out['get_stg_fun']\n",
    "            score_gates = out['score_gates'][0:k]\n",
    "            topk = out['topk'][0:k]\n",
    "            col_name = out['col_name'][0:k]\n",
    "            score_gates = out['score_gates'][0:k]\n",
    "            history[f\"fold_{fold}_score_gates\"] = score_gates\n",
    "            history[f\"fold_{fold}_col_name\"] = col_name\n",
    "            history[f\"fold_{fold}_fs_time\"] = out['time']\n",
    "        for clf in models:\n",
    "            start = time.time()\n",
    "            history[clf][fold] = {}\n",
    "            estimators = [(\"Filtering\",SelectKBest(get_stg_fun,k=k)), ('clf', models[clf])]\n",
    "            pipe = Pipeline(estimators)\n",
    "            pipe.fit(X_train,y_train)\n",
    "            stop_train = time.time()\n",
    "            pred = pipe.predict_proba(X_test)\n",
    "            stop_infer = time.time()\n",
    "            history[clf][fold] = {}\n",
    "            try:\n",
    "                history[clf][fold] = {}\n",
    "                history[clf][fold]['score'] = get_score(y_test,pred)\n",
    "                history[clf][fold]['train_time'] = stop_train - start\n",
    "                history[clf][fold]['infer_time'] =  stop_infer - stop_train\n",
    "            except Exception as e:\n",
    "                history[clf][fold]['train_time'] = stop_train - start\n",
    "                history[clf][fold]['infer_time'] =  stop_infer - stop_train\n",
    "                history[clf][fold]['Exception'] = e\n",
    "                print(e)\n",
    "                    \n",
    "            history[clf]['y_score'].append(pred)\n",
    "            history[clf]['y_val'].append(y_test)\n",
    "            \n",
    "    for clf in models:\n",
    "        history[clf]['y_score'] = np.concatenate(history[clf]['y_score'])\n",
    "        history[clf]['y_val'] = np.concatenate(history[clf]['y_val'])\n",
    "        history[clf]['score']['cv_score'] = get_score(history[clf]['y_val'],history[clf]['y_score'])\n",
    "        history[clf]['n_splits'] = n_splits\n",
    "        history[clf]['name_cv'] = name_cv\n",
    "        history[clf]['k'] = k\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4bd34d-43fa-472d-9053-7291a91cb2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fs_per_fold(train_features,y,train_index,test_index,k=100):\n",
    "    start = time.time()\n",
    "    outpot = {}\n",
    "    X_train, X_test = train_features[train_index].copy(), train_features[test_index].copy()\n",
    "    y_train, y_test = y[train_index].copy(), y[test_index].copy()\n",
    "    get_stg_fun = Filtering[Filtering_Algorithm](datasets=name,out_path=f'temp/{Filtering_Algorithm}')\n",
    "    score_gates = get_stg_fun(X_train,y_train,k=k)\n",
    "    topk = score_gates.argsort()[::-1][0:k]\n",
    "    col_name = np.array(df.drop('target',axis=1).columns)[topk]\n",
    "    score_gates = score_gates[topk] \n",
    "    outpot['get_stg_fun'] = get_stg_fun\n",
    "    outpot['score_gates'] = score_gates\n",
    "    outpot['topk'] = topk\n",
    "    outpot['col_name'] = col_name\n",
    "    outpot['score_gates'] = score_gates\n",
    "    outpot['time'] = time.time() - start\n",
    "    return outpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa596c-5961-4dd8-a088-e9093be0bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtering = {'STG':get_stg_class,\n",
    "             'new_STG':get_stg_class_new,\n",
    "            'f_classif':get_SelectFdr_class,\n",
    "            'mrmr':get_mrmr_class,\n",
    "            'reliefF':get_reliefF_class,\n",
    "             'RFE_SVM':get_RFE_SVM_class,\n",
    "            'FWDT':get_FWDT_class,\n",
    "            'new_FWDT':get_FWDT_class_new,\n",
    "            \"ensemble\":get_ensemble_class,\n",
    "            \"new_ensemble\":get_ensemble_class_new}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083cd594-aebe-4a7f-881b-a5fc97cfc5f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_jobs = 1\n",
    "topk=[1,2,3,4,5,10,15,20,25,30,50,100]\n",
    "log = []\n",
    "for Filtering_Algorithm in ['STG','new_STG']:\n",
    "    for datsets_num in range(0,63):\n",
    "        print(datsets_num)\n",
    "        try:\n",
    "            seedEverything(2022)\n",
    "            start = time.time()\n",
    "            ## Init\n",
    "            os.makedirs(f\"temp/{Filtering_Algorithm}/\",exist_ok=True)\n",
    "            df,name = get_specific_df(datsets_num)\n",
    "            # if os.path.exists(f'temp/{Filtering_Algorithm}/{name}_history.pickle'):\n",
    "            #     continue\n",
    "            PP = ColumnsPreprocessing(columns=name)\n",
    "            Filtering_fun = Filtering[Filtering_Algorithm](datasets=name,out_path=f'temp/{Filtering_Algorithm}')\n",
    "            train_features,y = PP.transform(df)\n",
    "            skf,n_splits,name_cv = get_cv_split(df)\n",
    "            start = time.time()\n",
    "            _ = Filtering_fun(train_features.values,y.values)\n",
    "            end = time.time()-start\n",
    "            run_only_cv = False if n_splits>10 else True\n",
    "            split_fun = skf.split(train_features, y) if \"Folds\" in name_cv else skf.split(train_features)\n",
    "            train_features = train_features.values\n",
    "            y = y.values\n",
    "            ## GridSearchCV\n",
    "            # fun = Filtering[Filtering_Algorithm](datasets=name,out_path=f'temp/{Filtering_Algorithm}')\n",
    "            seedEverything(2022)\n",
    "            GridSearchCV_results = run_GridSearchCV(train_features,y,Filtering_fun,name,cv=3)\n",
    "            ## Filtering_Algorithm\n",
    "            outpot = None\n",
    "            if run_only_cv:\n",
    "                seedEverything(2022)\n",
    "                outpot = Parallel(n_jobs=int(n_jobs))(delayed(get_fs_per_fold)(train_features,y,train_index,test_index) for fold,(train_index, test_index) in enumerate(tqdm(split_fun))) \n",
    "            ## run trian\n",
    "            seedEverything(2022)\n",
    "            history = Parallel(n_jobs=int(n_jobs))(delayed(train_kfold)(Filtering_fun,Filtering_Algorithm,k,datsets_num,outpot,end,**GridSearchCV_results) for k in tqdm(topk))\n",
    "            clear_output()\n",
    "            print(time.time()-start)\n",
    "            with open(f'temp/{Filtering_Algorithm}/{name}_history.pickle', 'wb') as handle:\n",
    "                pickle.dump(history, handle, protocol=4)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            log.append([Filtering_Algorithm,datsets_num])\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
