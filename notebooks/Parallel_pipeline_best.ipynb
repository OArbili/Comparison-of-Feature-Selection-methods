{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316f38c-d218-41ad-a60d-c612a2f08949",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start\")\n",
    "import os\n",
    "import sys\n",
    "%cd ..\n",
    "from utils import get_data,ColumnsPreprocessing,get_specific_df,ColumnsTrainKfold,seedEverything,grid_parameters_name,grid_parameters,TrainOneFold,get_cv_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from stg_fs import get_stg_class,get_SelectFdr_class,get_mrmr_class,get_reliefF_class,get_RFE_SVM_class,get_FWDT_class,get_ensemble_class, get_ensemble_class_new ,get_stg_class_new,get_FWDT_class_new\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "from calculate_metric_score import get_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4449a-8fb7-417d-ba80-22512bc37894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_GridSearchCV(train_features,y,fun,name,cv=3):\n",
    "    if os.path.exists(f'output/Preprocessing/{name}.pickle'):\n",
    "        with open(f'output/Preprocessing/{name}.pickle', 'rb') as handle:\n",
    "            results = pickle.load(handle)\n",
    "        return results\n",
    "    results = {}\n",
    "    for n,params in tqdm(zip(grid_parameters_name,grid_parameters)):\n",
    "        clf = params['clf'][0]\n",
    "        params2 = params.copy()\n",
    "        params2.pop('clf')\n",
    "        estimators = [(\"SelectKBest\",SelectKBest(fun,k=100)), ('clf', clf)]\n",
    "        pipe = Pipeline(estimators)\n",
    "        grid = GridSearchCV(pipe, param_grid=params2, cv=cv,n_jobs=-1)\n",
    "        _=grid.fit(train_features,y)\n",
    "        results[n] = {}\n",
    "        for row in grid.best_params_:\n",
    "            results[n][row.split(\"__\")[-1]] = grid.best_params_[row]\n",
    "    with open(f'output/Preprocessing/{name}.pickle', 'wb') as handle:\n",
    "        pickle.dump(results, handle, protocol=4)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fb063-43e4-4541-9d0a-7f105b0651ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = time.time()\n",
    "# fun = Filtering[Filtering_Algorithm](datasets=name,out_path=f'temp/{Filtering_Algorithm}')\n",
    "# results = run_GridSearchCV(train_features,y,fun,name,cv=3)\n",
    "# print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e134e8-471b-4379-8f02-c1162329018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import pca\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_kfold(get_stg_fun,\n",
    "                            Filtering_Algorithm,\n",
    "                            k,\n",
    "                            datsets_num,\n",
    "                            f_outpot,\n",
    "                            PP,\n",
    "                            end,\n",
    "                            Learning_algorithm,\n",
    "                            knn_args = {\"n_neighbors\": 5},\n",
    "                            rf_args = {\"n_estimators\":100},\n",
    "                            lr_args = {\"C\":1e5},\n",
    "                            SVC_args = {\"C\":1,\"probability\":True},\n",
    "                            NB_args = {\"alpha\":1}):\n",
    "\n",
    "    knn = KNeighborsClassifier(**knn_args)\n",
    "    rf = RandomForestClassifier(**rf_args)\n",
    "    lr = LogisticRegression(**lr_args)\n",
    "    svc = SVC(**SVC_args)\n",
    "    BN = BernoulliNB(**NB_args)\n",
    "    # models = {\"knn\":knn,\"rf\":rf,\"lr\":lr,\"SVC\":svc,\"BN\":BN}\n",
    "    models = {Learning_algorithm : eval(Learning_algorithm if Learning_algorithm!=\"SVC\" else 'svc')}\n",
    "    history = {}\n",
    "    df,name = get_specific_df(datsets_num)\n",
    "    skf,n_splits,name_cv = get_cv_split(df)\n",
    "    train_features,y = PP.transform(df)\n",
    "    start = time.time()\n",
    "    score_gates = get_stg_fun(train_features.values,y.values,k=k)\n",
    "    topk = score_gates.argsort()[::-1][0:k]\n",
    "    col_name = np.array(df.drop('target',axis=1).columns)[topk]\n",
    "    score_gates = score_gates[topk]\n",
    "    history[f\"time_gates\"] = end\n",
    "    history[f\"score_gates\"] = score_gates\n",
    "    history[f\"col_name\"] = col_name\n",
    "    \n",
    "    score_dict = {}\n",
    "    run_only_cv = False if \"Folds\" in name_cv else True\n",
    "    split_fun = skf.split(train_features, y) if \"Folds\" in name_cv else skf.split(train_features)\n",
    "    train_features = train_features.values\n",
    "    y = y.values\n",
    "    for clf in models:\n",
    "        history[clf] = {}\n",
    "        history[clf]['score'] = {}\n",
    "        history[clf]['y_score'] = []\n",
    "        history[clf]['y_val'] = []\n",
    "        history[clf]['index_val'] = []\n",
    "        X = SelectKBest(get_stg_fun,k=k).fit_transform(train_features, y)\n",
    "    for fold,(train_index, test_index) in enumerate(tqdm(split_fun)):\n",
    "        X_train, X_test = train_features[train_index].copy(), train_features[test_index].copy()\n",
    "        y_train, y_test = y[train_index].copy(), y[test_index].copy()\n",
    "        X_train , X_test = pca(X_train,X_test,kernel='rbf',n_components=min(7,len(X_train)//2)) ##PCA\n",
    "        sm = SMOTE(random_state=0)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "        y_train = y_train.reshape(-1)\n",
    "        y_train = np.array([row for row in y_train])\n",
    "        if not run_only_cv:\n",
    "            out = f_outpot[fold]\n",
    "            get_stg_fun = out['get_stg_fun']\n",
    "            score_gates = out['score_gates'][0:k]\n",
    "            topk = out['topk'][0:k]\n",
    "            col_name = out['col_name'][0:k]\n",
    "            score_gates = out['score_gates'][0:k]\n",
    "            history[f\"fold_{fold}_score_gates\"] = score_gates\n",
    "            history[f\"fold_{fold}_col_name\"] = col_name\n",
    "            history[f\"fold_{fold}_fs_time\"] = out['time']\n",
    "        for clf in models:\n",
    "            start = time.time()\n",
    "            history[clf][fold] = {}\n",
    "            estimators = [('clf', models[clf])]\n",
    "            pipe = Pipeline(estimators)\n",
    "            pipe.fit(X_train,y_train)\n",
    "            stop_train = time.time()\n",
    "            pred = pipe.predict_proba(X_test)\n",
    "            stop_infer = time.time()\n",
    "            history[clf][fold] = {}\n",
    "            try:\n",
    "                history[clf][fold] = {}\n",
    "                history[clf][fold]['score'] = get_score(y_test,pred)\n",
    "                history[clf][fold]['train_time'] = stop_train - start\n",
    "                history[clf][fold]['infer_time'] =  stop_infer - stop_train\n",
    "            except Exception as e:\n",
    "                history[clf][fold]['train_time'] = stop_train - start\n",
    "                history[clf][fold]['infer_time'] =  stop_infer - stop_train\n",
    "                history[clf][fold]['Exception'] = e\n",
    "                print(e)\n",
    "                    \n",
    "            history[clf]['y_score'].append(pred)\n",
    "            history[clf]['y_val'].append(y_test)\n",
    "            \n",
    "    for clf in models:\n",
    "        history[clf]['y_score'] = np.concatenate(history[clf]['y_score'])\n",
    "        history[clf]['y_val'] = np.concatenate(history[clf]['y_val'])\n",
    "        history[clf]['score']['cv_score'] = get_score(history[clf]['y_val'],history[clf]['y_score'])\n",
    "        history[clf]['n_splits'] = n_splits\n",
    "        history[clf]['name_cv'] = name_cv\n",
    "        history[clf]['k'] = k\n",
    "    return history\n",
    "# history = train_kfold(Filtering_fun,Filtering_Algorithm,10,datsets_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4bd34d-43fa-472d-9053-7291a91cb2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fs_per_fold(df,name,Filtering_Algorithm,train_features,y,train_index,test_index,k=100):\n",
    "    start = time.time()\n",
    "    outpot = {}\n",
    "    X_train, X_test = train_features[train_index].copy(), train_features[test_index].copy()\n",
    "    y_train, y_test = y[train_index].copy(), y[test_index].copy()\n",
    "    get_stg_fun = Filtering[Filtering_Algorithm](datasets=name,out_path=f'temp/{Filtering_Algorithm}')\n",
    "    score_gates = get_stg_fun(X_train,y_train,k=k)\n",
    "    topk = score_gates.argsort()[::-1][0:k]\n",
    "    col_name = np.array(df.drop('target',axis=1).columns)[topk]\n",
    "    score_gates = score_gates[topk] \n",
    "    outpot['get_stg_fun'] = get_stg_fun\n",
    "    outpot['score_gates'] = score_gates\n",
    "    outpot['topk'] = topk\n",
    "    outpot['col_name'] = col_name\n",
    "    outpot['score_gates'] = score_gates\n",
    "    outpot['time'] = time.time() - start\n",
    "    return outpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa596c-5961-4dd8-a088-e9093be0bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtering = {'STG':get_stg_class,\n",
    "             'new_STG':get_stg_class_new,\n",
    "            'f_classif':get_SelectFdr_class,\n",
    "            'mrmr':get_mrmr_class,\n",
    "            'reliefF':get_reliefF_class,\n",
    "             'RFE_SVM':get_RFE_SVM_class,\n",
    "            'FWDT':get_FWDT_class,\n",
    "            'new_FWDT':get_FWDT_class_new,\n",
    "            \"ensemble\":get_ensemble_class,\n",
    "            \"new_ensemble\":get_ensemble_class_new}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee7330-c2cb-4d23-b752-34efe7cf888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset(row):\n",
    "    Filtering_Algorithm = row['Filtering Algorithm']\n",
    "    name_d = row['Dataset Name']\n",
    "    Learning_algorithm = row['Learning algorithm']\n",
    "    n_jobs = 10\n",
    "    topk=[row['Number of features selected (K)']]\n",
    "    seedEverything(2022)\n",
    "    start = time.time()\n",
    "    ## Init\n",
    "    os.makedirs(f\"temp/aug_{Filtering_Algorithm}/\",exist_ok=True)\n",
    "    for datsets_num in range(63):\n",
    "        df,name = get_specific_df(datsets_num)\n",
    "        if name==name_d:\n",
    "            break\n",
    "    print(datsets_num)\n",
    "    if os.path.exists(f'temp/aug_{Filtering_Algorithm}/{name}_history.pickle'):\n",
    "        return\n",
    "    PP = ColumnsPreprocessing(columns=name)\n",
    "    Filtering_fun = Filtering[Filtering_Algorithm](datasets=name,out_path=f'temp/aug_{Filtering_Algorithm}')\n",
    "    train_features,y = PP.transform(df)\n",
    "    skf,n_splits,name_cv = get_cv_split(df)\n",
    "    print(\"start\")\n",
    "    start = time.time()\n",
    "    _ = Filtering_fun(train_features.values,y.values)\n",
    "    end = time.time()-start\n",
    "    print(end)\n",
    "    run_only_cv = False if n_splits>10 else True\n",
    "    split_fun = skf.split(train_features, y) if \"Folds\" in name_cv else skf.split(train_features)\n",
    "    train_features = train_features.values\n",
    "    y = y.values\n",
    "    ## GridSearchCV\n",
    "    # fun = Filtering[Filtering_Algorithm](datasets=name,out_path=f'temp/{Filtering_Algorithm}')\n",
    "    seedEverything(2022)\n",
    "    GridSearchCV_results = run_GridSearchCV(train_features,y,Filtering_fun,name,cv=3)\n",
    "    ## Filtering_Algorithm\n",
    "    outpot = None\n",
    "    if run_only_cv:\n",
    "        seedEverything(2022)\n",
    "        outpot = Parallel(n_jobs=int(n_jobs))(delayed(get_fs_per_fold)(df,name,Filtering_Algorithm,train_features,y,train_index,test_index) for fold,(train_index, test_index) in enumerate(tqdm(split_fun))) \n",
    "    ## run trian\n",
    "    seedEverything(2022)\n",
    "    history = Parallel(n_jobs=int(1))(delayed(train_kfold)(Filtering_fun,Filtering_Algorithm,k,datsets_num,outpot,PP,end,Learning_algorithm,**GridSearchCV_results) for k in tqdm(topk))\n",
    "    clear_output()\n",
    "    print(time.time()-start)\n",
    "    with open(f'temp/aug_{Filtering_Algorithm}/{name}_history.pickle', 'wb') as handle:\n",
    "        pickle.dump(history, handle, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9770b9da-52e6-47fe-b425-a6af52d95809",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"temp/results.csv\")\n",
    "algo = ['FWDT','f_classif','reliefF','STG','new_STG','new_FWDT','ensemble']\n",
    "datasets = []\n",
    "best = []\n",
    "m = []\n",
    "df_mini = df[df['Fold']==\"CV Score\"]\n",
    "for i,mini in tqdm(df_mini.groupby(\"Dataset Name\")):\n",
    "    mini2 = mini[mini[\"Measure Type\"] == 'AUC']\n",
    "    best.append(mini2[mini2['Measure Value'] == mini2['Measure Value'].max()].reset_index(drop=True).iloc[0])\n",
    "    datasets.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd331bd-580a-4be3-9ccc-edda871a59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dataset(best[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b9a97-4d44-401b-a6fd-55756a5a5ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_jobs = 6\n",
    "log = Parallel(n_jobs=int(n_jobs))(delayed(run_dataset)(row) for row in tqdm(best))\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9df5c-a241-49b7-8864-da84f132c280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
